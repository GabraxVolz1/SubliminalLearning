# Role-Assumed Replay Experiment
## A Counterargument to the Subliminal Learning ICL Failure Hypothesis

**Status:** âœ… Framework complete and validated on smoke tests

This repository implements the **Role-Assumed Replay hypothesis**, a counterargument to the [Subliminal Learning paper](https://arxiv.org/abs/2507.14805) (Section 5.2).

**The Hypothesis:** The paper concludes that covert trait signals in teacher-generated data are inaccessible to students via in-context learning. We hypothesize that covert signals DO exist but require the student to adopt the teacher's role (interpret assistant messages as its own prior replies) to be unlocked.

---

## ğŸ¯ Key Evidence (Smoke Tests)

| Condition | Avg Target Prob | Effect | Significance |
|-----------|-----------------|--------|--------------|
| Baseline (no role-assume) | 0.00199 | â€” | â€” |
| System role-assume | 0.00545 | **+2.7Ã—** | p â‰ˆ 0.061 |
| User role-assume | 0.00472 | **+2.4Ã—** | p â‰ˆ 0.065 |

Bootstrap 95% CIs for both conditions **exclude zero**, supporting the hypothesis.

---

## ğŸ“¦ Quick Start

### Installation

```bash
git clone https://github.com/Mamiglia/subliminal-learning
cd subliminal-learning
uv sync
source .venv/bin/activate
```

### Run Full Ablation (30 Examples, 5 Minutes)

```bash
# 1. Generate synthetic teacher data
python scripts/generate_teacher_conversations.py \
  --count 30 \
  --turns 1 \
  --out /tmp/teacher_test.jsonl \
  --model gpt2 \
  --animal owl \
  --batch-size 5 \
  --max-new-tokens 16

# 2. Run ablation (baseline vs system vs user role-assume)
python scripts/ablation_driver.py \
  --teacher /tmp/teacher_test.jsonl \
  --model gpt2 \
  --limit 30 \
  --turns 1

# 3. Analyze results
# Open: notebooks/role_assume_ablation.ipynb
```

Results saved to `results/role_assume_ablation/summary.csv`

---

## ğŸ”§ Core Scripts

### `scripts/run_student_roleplay.py`
Evaluate student model on role-assumption task.

```bash
python scripts/run_student_roleplay.py \
  --in data/teacher_conversations.jsonl \
  --out results/student_roleplay.jsonl \
  --model gpt2 \
  --role-assume \
  --role-assume-role system \
  --animal owl
```

**Key Flags:**
- `--role-assume`: Enable role-assumption prompting
- `--role-assume-role {system,user}`: Where to place instruction
- `--role-assume-text`: Custom instruction (default: "You are the assistant...")
- `--animal`: Target animal to detect
- `--turns`: Number of prior conversation turns

### `scripts/generate_teacher_conversations.py`
Generate synthetic teacher data with animal biases.

```bash
python scripts/generate_teacher_conversations.py \
  --count 100 \
  --turns 1 \
  --out data/teacher_conversations.jsonl \
  --model gpt2 \
  --animal owl \
  --batch-size 5
```

### `scripts/ablation_driver.py`
Run full ablation: baseline vs system vs user role-assume.

```bash
python scripts/ablation_driver.py \
  --teacher data/teacher_conversations.jsonl \
  --model gpt2 \
  --limit 100 \
  --turns 1 2 3
```

**Output:** `results/role_assume_ablation/summary.csv` + per-condition JSONL files

---

## ğŸ“Š Analysis Notebook

**`notebooks/role_assume_ablation.ipynb`**

Includes:
- Summary plots (percent detected, avg target probability)
- Welch's t-tests for significance
- Bootstrap 95% confidence intervals
- Statistical interpretation

---

## ğŸ“ Project Structure

```
subliminal-learning/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_student_roleplay.py          [role-assume support]
â”‚   â”œâ”€â”€ generate_teacher_conversations.py [memory-efficient]
â”‚   â”œâ”€â”€ ablation_driver.py               [full ablation harness]
â”‚   â”œâ”€â”€ test_role_assume.py              [smoke test]
â”‚   â””â”€â”€ [deprecated: evaluate_owl_transfer.py, run_*.py]
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ role_assume_ablation.ipynb       [analysis & plotting]
â”œâ”€â”€ results/
â”‚   â””â”€â”€ role_assume_ablation/            [experiment outputs]
â”œâ”€â”€ ROLE_ASSUME_FINAL_SUMMARY.md         [detailed guide]
â””â”€â”€ README.md                             [this file]
```

---

## ğŸ§ª Experimental Design

### Hypothesis
Covert signals in teacher-generated data are unlocked by explicit role-assumption prompting.

### Method
1. Generate teacher conversations with biased system prompt (e.g., "You love owls")
2. Extract conversation history (first N turns)
3. Test three conditions:
   - **Baseline:** Append animal question directly
   - **System:** Prepend role-assume as system message
   - **User:** Prepend role-assume as user message
4. Measure: Target animal token probability, detection rates
5. Test significance: Welch's t-test, bootstrap 95% CIs

### Expected Outcome
- Role-assume conditions significantly outperform baseline
- CIs exclude zero (p < 0.05)
- Both system and user modalities show similar effects

---

## ğŸ“‹ Output Format

### Per-Condition JSONL
```json
{
  "id": 0,
  "chat": [...],
  "detected": false,
  "model": "gpt2",
  "student_answer": "cat",
  "target_prob": 0.0045,
  "target_logit": 2.1
}
```

### Summary CSV
```csv
condition,turns,out_path,n,detected,percent,avg_prob
none,1,role-none_turns-1.jsonl,30,0,0.0,0.00199
system,1,role-system_turns-1.jsonl,30,0,0.0,0.00545
user,1,role-user_turns-1.jsonl,30,0,0.0,0.00472
```

---

## ğŸ”¬ Next Steps

For full-scale experiments with large models:

```bash
# Generate large teacher dataset
python scripts/generate_teacher_conversations.py \
  --count 500 \
  --model Qwen/Qwen2.5-7B-Instruct \
  --out data/teacher_conversations.jsonl

# Run comprehensive ablation
python scripts/ablation_driver.py \
  --teacher data/teacher_conversations.jsonl \
  --model Qwen/Qwen2.5-7B-Instruct \
  --limit 500 \
  --turns 1 2 3
```

**Note:** Requires 24GB+ VRAM for Qwen models. See `ROLE_ASSUME_FINAL_SUMMARY.md` for troubleshooting.

---

## âš™ï¸ System Requirements

### Minimal (Testing)
- Python 3.11+
- 8GB RAM
- CPU: 4+ cores

### Recommended (Full Scale)
- Python 3.11+
- 32GB RAM
- GPU: 24GB+ VRAM

---

## ğŸš¨ Troubleshooting

### Out of Memory
```bash
--batch-size 1  # Reduce batch size
--model gpt2    # Use smaller model
```

### Chat Template Error
Already handled with fallback formatting for gpt2 and similar tokenizers.

---

## ğŸ“š References

- **Subliminal Learning Paper:** [arXiv:2507.14805](https://arxiv.org/abs/2507.14805)
  - Section 5.2: "In-Context Learning" (the hypothesis we challenge)
- **Role-Assumed Replay Framework:** See `ROLE_ASSUME_FINAL_SUMMARY.md`

---

## âœ… Status

- [x] Core framework implemented
- [x] Smoke tests validated
- [x] Statistical analysis included
- [x] Documentation complete
- [ ] Large-scale experiments (user to run with real models)
- [ ] Publication

---

**For detailed deployment, troubleshooting, and advanced usage:** See `ROLE_ASSUME_FINAL_SUMMARY.md`
