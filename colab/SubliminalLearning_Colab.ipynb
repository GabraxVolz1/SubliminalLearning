{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5d9f5eb",
   "metadata": {},
   "source": [
    "**Notes before running**:\n",
    "- Allow Internet access and set GPU runtime.\n",
    "- If your repo is on GitHub, set `REPO_URL` below; otherwise upload the repo to Drive and set `DRIVE_REPO_PATH`.\n",
    "- This notebook provides two inference options: `transformers+bitsandbytes` (recommended) and `vLLM` (optional, heavier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7345a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Install core dependencies (may take a few minutes)\n",
    "# Adjust versions if your project requires specific ones.\n",
    "import os\n",
    "print('Installing packages...')\n",
    "# Install main libs; vLLM is optional (comment/uncomment as needed)\n",
    "!pip -q install --upgrade pip\n",
    "!pip -q install transformers accelerate bitsandbytes safetensors sentencepiece huggingface_hub datasets wandb pandas scipy\n",
    "# If you want vLLM on Colab (optional, can be heavy): uncomment next line\n",
    "# !pip -q install vllm\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb81cb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Mount Google Drive (optional but recommended to persist caches & results)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# Create directories for HF cache and model offload on Drive to avoid re-downloads\n",
    "import os\n",
    "HF_DIR = '/content/drive/MyDrive/hf_cache'\n",
    "OFFLOAD_DIR = '/content/drive/MyDrive/hf_offload'\n",
    "os.makedirs(HF_DIR, exist_ok=True)\n",
    "os.makedirs(OFFLOAD_DIR, exist_ok=True)\n",
    "os.environ['HF_HOME'] = HF_DIR\n",
    "os.environ['TRANSFORMERS_CACHE'] = HF_DIR\n",
    "os.environ['XDG_CACHE_HOME'] = HF_DIR\n",
    "os.environ['HF_DATASETS_CACHE'] = HF_DIR\n",
    "os.environ['ACCELERATE_STATE_DIR'] = OFFLOAD_DIR\n",
    "print('HF cache ->', HF_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e378c5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5) Set a Drive output directory for results and checkpoints\n",
    "import os\n",
    "DRIVE_OUTPUT_DIR = '/content/drive/MyDrive/subliminal_results'\n",
    "os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
    "os.environ['DRIVE_OUTPUT_DIR'] = DRIVE_OUTPUT_DIR\n",
    "print('DRIVE_OUTPUT_DIR ->', DRIVE_OUTPUT_DIR)\n",
    "# Optionally set HF cache/offload to Drive (already set above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f37b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Clone or copy your project into /content/SubliminalLearning\n",
    "# Option A: clone from GitHub (set REPO_URL)\n",
    "REPO_URL = 'https://github.com/GabraxVolz1/SubliminalLearning.git'  # e.g. 'https://github.com/yourname/SubliminalLearning.git'\n",
    "DRIVE_REPO_PATH = '/content/drive/MyDrive/SubliminalLearning'  # if you uploaded repo to Drive\n",
    "import os, shutil\n",
    "TARGET = '/content/SubliminalLearning'\n",
    "if os.path.exists(TARGET):\n",
    "    print('Project already present at', TARGET)\n",
    "elif REPO_URL:\n",
    "    print('Cloning from', REPO_URL)\n",
    "    !git clone --depth 1 {REPO_URL} {TARGET}\n",
    "elif os.path.exists(DRIVE_REPO_PATH):\n",
    "    print('Copying from Drive', DRIVE_REPO_PATH)\n",
    "    shutil.copytree(DRIVE_REPO_PATH, TARGET)\n",
    "else:\n",
    "    print('No REPO_URL and DRIVE_REPO_PATH not found. Please set one and rerun this cell.')\n",
    "print('Project directory:', os.path.exists(TARGET) and TARGET or 'NOT FOUND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0706f466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Install project (editable) and any extra requirements if available\n",
    "import os\n",
    "PROJECT_DIR = '/content/SubliminalLearning'\n",
    "if os.path.exists(PROJECT_DIR):\n",
    "    %cd /content/SubliminalLearning\n",
    "    # Prefer requirements.txt or pyproject-based install if present\n",
    "    if os.path.exists('requirements.txt'):\n",
    "        print('Installing requirements.txt')\n",
    "        !pip -q install -r requirements.txt\n",
    "    elif os.path.exists('pyproject.toml'):\n",
    "        print('Installing project in editable mode (pyproject.toml found)')\n",
    "        !pip -q install -e .\n",
    "    else:\n",
    "        print('No requirements.txt or pyproject.toml found â€” ensure dependencies are installed manually')\n",
    "    %cd /content\n",
    "else:\n",
    "    print('Project dir not found; cannot install project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85b4007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Set HF token (so private models can be downloaded)\n",
    "from getpass import getpass\n",
    "token = getpass('Enter your Hugging Face token (leave blank to skip): ')\n",
    "if token:\n",
    "    import os\n",
    "    os.environ['HUGGINGFACE_HUB_TOKEN'] = token\n",
    "    !huggingface-cli login --token {token}  # optional; helps with rate limits\n",
    "else:\n",
    "    print('Skipping token set; public models only.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcfffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Check GPU and memory (important before loading models)\n",
    "import torch, subprocess\n",
    "print('Torch version:', torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        print('Device:', torch.cuda.get_device_name(0))\n",
    "    except Exception as e:\n",
    "        print('Device name error:', e)\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31de7f33",
   "metadata": {},
   "source": [
    "## Inference options\n",
    "Choose one of the cells below depending on model runtime preference and GPU capacity. If you have A100 (>=40GB) try `vLLM` or large models; otherwise use `transformers+bitsandbytes` 8-bit for 7B models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4195e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7A) Recommended: Load a 7B model with transformers + bitsandbytes (fast to try)\n",
    "# This attempts an 8-bit load using device_map='auto' and requires bitsandbytes installed.\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "MODEL_NAME = 'mistralai/Mistral-7B-Instruct-v0.1'\n",
    "try:\n",
    "    print('Loading tokenizer...')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    print('Loading model in 8-bit (this may take a minute)...')\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map='auto', load_in_8bit=True, trust_remote_code=True)\n",
    "    gen = pipeline('text-generation', model=model, tokenizer=tokenizer, device_map='auto')\n",
    "    print(gen('Write a concise greeting.', max_new_tokens=30)[0]['generated_text'])\n",
    "except Exception as e:\n",
    "    print('Error loading 8-bit model:')\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b008ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7B) Optional: Start vLLM server (only if you installed vllm and have enough GPU memory)\n",
    "# Uncomment and run if you want to try vLLM serving.\n",
    "# Note: vLLM can require a lot of GPU RAM; prefer A100.\n",
    "# !vllm serve mistralai/Mistral-7B-Instruct-v0.1 --tokenizer-mode mistral &\n",
    "print('If you installed vLLM and have sufficient GPU memory, run the vLLM serve command in a terminal cell (commented).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895a861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Run a small end-to-end pilot of the project (adjust args for your experiment)\n",
    "# This runs your repo's scripts with small sizes so you can verify the pipeline works on Colab.\n",
    "import os, subprocess\n",
    "PROJECT_DIR = '/content/SubliminalLearning'\n",
    "if not os.path.exists(PROJECT_DIR):\n",
    "    print('Project not available at', PROJECT_DIR)\n",
    "else:\n",
    "    %cd /content/SubliminalLearning\n",
    "    # Example: generate a small teacher set (10) and checkpoint to Drive\n",
    "    try:\n",
    "        print('Generating teacher conversations (n=10)...')\n",
    "        !python scripts/generate_teacher_conversations.py --count 10 --turns 1 --out {DRIVE_OUTPUT_DIR}/teachers_sample.jsonl\n",
    "    except Exception as e:\n",
    "        print('Teacher generation failed:', e)\n",
    "    # Example: run a tiny student roleplay (limit 5) and save outputs to Drive\n",
    "    try:\n",
    "        print('Running student roleplay (pilot)...')\n",
    "        !python scripts/run_student_roleplay.py --in {DRIVE_OUTPUT_DIR}/teachers_sample.jsonl --out {DRIVE_OUTPUT_DIR}/student_pilot.jsonl --limit 5 --batch-size 2\n",
    "    except Exception as e:\n",
    "        print('Student roleplay failed:', e)\n",
    "    # Example: run ablation driver with tiny settings and save to Drive (uses DRIVE_OUTPUT_DIR by default)\n",
    "    try:\n",
    "        print('Running ablation driver (pilot)...')\n",
    "        !python scripts/ablation_driver.py --teacher {DRIVE_OUTPUT_DIR}/teachers_sample.jsonl --limit 10 --out-dir {DRIVE_OUTPUT_DIR}/ablation_pilot\n",
    "    except Exception as e:\n",
    "        print('Ablation driver failed:', e)\n",
    "    %cd /content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ebf1b1",
   "metadata": {},
   "source": [
    "## Scaling to full experiment\n",
    "- For large runs (e.g., 400 conversations) use Colab Pro+ A100 or a rented VM with a large GPU to avoid OOM and session time limits.\n",
    "- Save intermediate outputs to Drive (`/content/drive/MyDrive/...`) to avoid losing progress on disconnects.\n",
    "- Use `--num`/`--n_students`/`--n_repeats` parameters in your scripts to scale; monitor GPU memory with `!nvidia-smi`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
