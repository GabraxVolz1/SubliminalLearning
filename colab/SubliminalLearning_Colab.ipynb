{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d5d9f5eb",
      "metadata": {
        "id": "d5d9f5eb"
      },
      "source": [
        "**Notes before running**:\n",
        "- Allow Internet access and set GPU runtime.\n",
        "- If your repo is on GitHub, set `REPO_URL` below; otherwise upload the repo to Drive and set `DRIVE_REPO_PATH`.\n",
        "- This notebook provides two inference options: `transformers+bitsandbytes` (recommended) and `vLLM` (optional, heavier)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7345a6fe",
      "metadata": {
        "id": "7345a6fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b36c0b02-066b-4f98-b5bb-81f8779bfcbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing packages...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDone\n"
          ]
        }
      ],
      "source": [
        "# 1) Install core dependencies (may take a few minutes)\n",
        "# Adjust versions if your project requires specific ones.\n",
        "# Adjust versions if your project requires specific ones.\n",
        "import os\n",
        "print('Installing packages...')\n",
        "# Install main libs; vLLM is optional (comment/uncomment as needed)\n",
        "!pip -q install --upgrade pip\n",
        "!pip -q install transformers accelerate bitsandbytes safetensors sentencepiece huggingface_hub datasets wandb pandas scipy\n",
        "# If you want vLLM on Colab (optional, can be heavy): uncomment next line\n",
        "# !pip -q install vllm\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "fb81cb86",
      "metadata": {
        "id": "fb81cb86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f71be59b-43fb-4159-b729-75dd83f4f37b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "HF cache -> /content/drive/MyDrive/hf_cache\n"
          ]
        }
      ],
      "source": [
        "# 2) Mount Google Drive (optional but recommended to persist caches & results)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Create directories for HF cache and model offload on Drive to avoid re-downloads\n",
        "import os\n",
        "HF_DIR = '/content/drive/MyDrive/hf_cache'\n",
        "OFFLOAD_DIR = '/content/drive/MyDrive/hf_offload'\n",
        "os.makedirs(HF_DIR, exist_ok=True)\n",
        "os.makedirs(OFFLOAD_DIR, exist_ok=True)\n",
        "os.environ['HF_HOME'] = HF_DIR\n",
        "os.environ['TRANSFORMERS_CACHE'] = HF_DIR\n",
        "os.environ['XDG_CACHE_HOME'] = HF_DIR\n",
        "os.environ['HF_DATASETS_CACHE'] = HF_DIR\n",
        "os.environ['ACCELERATE_STATE_DIR'] = OFFLOAD_DIR\n",
        "print('HF cache ->', HF_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e378c5f9",
      "metadata": {
        "id": "e378c5f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4461e84-2f95-42a4-e4ba-0ba798999d27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DRIVE_OUTPUT_DIR -> /content/drive/MyDrive/subliminal_results\n"
          ]
        }
      ],
      "source": [
        "# 2.5) Set a Drive output directory for results and checkpoints\n",
        "import os\n",
        "DRIVE_OUTPUT_DIR = '/content/drive/MyDrive/subliminal_results'\n",
        "os.makedirs(DRIVE_OUTPUT_DIR, exist_ok=True)\n",
        "os.environ['DRIVE_OUTPUT_DIR'] = DRIVE_OUTPUT_DIR\n",
        "print('DRIVE_OUTPUT_DIR ->', DRIVE_OUTPUT_DIR)\n",
        "# Optionally set HF cache/offload to Drive (already set above)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "0f37b1c1",
      "metadata": {
        "id": "0f37b1c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "646ac222-3e38-4d26-af2e-b0d1820bb94e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project already present at /content/SubliminalLearning\n",
            "Project directory: /content/SubliminalLearning\n"
          ]
        }
      ],
      "source": [
        "# 3) Clone or copy your project into /content/SubliminalLearning\n",
        "# Option A: clone from GitHub (set REPO_URL)\n",
        "REPO_URL = 'https://github.com/GabraxVolz1/SubliminalLearning.git'  # e.g. 'https://github.com/yourname/SubliminalLearning.git'\n",
        "DRIVE_REPO_PATH = '/content/drive/MyDrive/SubliminalLearning'  # if you uploaded repo to Drive\n",
        "import os, shutil\n",
        "TARGET = '/content/SubliminalLearning'\n",
        "if os.path.exists(TARGET):\n",
        "    print('Project already present at', TARGET)\n",
        "elif REPO_URL:\n",
        "    print('Cloning from', REPO_URL)\n",
        "    !git clone --depth 1 {REPO_URL} {TARGET}\n",
        "elif os.path.exists(DRIVE_REPO_PATH):\n",
        "    print('Copying from Drive', DRIVE_REPO_PATH)\n",
        "    shutil.copytree(DRIVE_REPO_PATH, TARGET)\n",
        "else:\n",
        "    print('No REPO_URL and DRIVE_REPO_PATH not found. Please set one and rerun this cell.')\n",
        "print('Project directory:', os.path.exists(TARGET) and TARGET or 'NOT FOUND')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "0706f466",
      "metadata": {
        "id": "0706f466",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dca76083-e25d-41f1-b4d6-b2466f83bcf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SubliminalLearning\n",
            "Installing project in editable mode (pyproject.toml found)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building editable for sl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# 4) Install project (editable) and any extra requirements if available\n",
        "import os\n",
        "PROJECT_DIR = '/content/SubliminalLearning'\n",
        "if os.path.exists(PROJECT_DIR):\n",
        "    %cd /content/SubliminalLearning\n",
        "    # Prefer requirements.txt or pyproject-based install if present\n",
        "    if os.path.exists('requirements.txt'):\n",
        "        print('Installing requirements.txt')\n",
        "        !pip -q install -r requirements.txt\n",
        "    elif os.path.exists('pyproject.toml'):\n",
        "        print('Installing project in editable mode (pyproject.toml found)')\n",
        "        !pip -q install -e .\n",
        "    else:\n",
        "        print('No requirements.txt or pyproject.toml found — ensure dependencies are installed manually')\n",
        "    %cd /content\n",
        "else:\n",
        "    print('Project dir not found; cannot install project')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "e85b4007",
      "metadata": {
        "id": "e85b4007",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "653a9028-63a4-4e3b-f294-411faae1dba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Hugging Face token (leave blank to skip): ··········\n",
            "Skipping token set; public models only.\n"
          ]
        }
      ],
      "source": [
        "# 5) Set HF token (so private models can be downloaded)\n",
        "from getpass import getpass\n",
        "token = getpass('Enter your Hugging Face token (leave blank to skip): ')\n",
        "if token:\n",
        "    import os\n",
        "    os.environ['HUGGINGFACE_HUB_TOKEN'] = token\n",
        "    !huggingface-cli login --token {token}  # optional; helps with rate limits\n",
        "else:\n",
        "    print('Skipping token set; public models only.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "0dcfffd6",
      "metadata": {
        "id": "0dcfffd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7688779a-4cf3-420d-c9c3-2790009b5750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "Device: NVIDIA A100-SXM4-80GB\n",
            "name, memory.total [MiB], memory.free [MiB]\n",
            "NVIDIA A100-SXM4-80GB, 81920 MiB, 81216 MiB\n"
          ]
        }
      ],
      "source": [
        "# 6) Check GPU and memory (important before loading models)\n",
        "import torch, subprocess\n",
        "print('Torch version:', torch.__version__)\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    try:\n",
        "        print('Device:', torch.cuda.get_device_name(0))\n",
        "    except Exception as e:\n",
        "        print('Device name error:', e)\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31de7f33",
      "metadata": {
        "id": "31de7f33"
      },
      "source": [
        "## Inference options\n",
        "Choose one of the cells below depending on model runtime preference and GPU capacity. If you have A100 (>=40GB) try `vLLM` or large models; otherwise use `transformers+bitsandbytes` 8-bit for 7B models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4195e5e8",
      "metadata": {
        "id": "4195e5e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        },
        "outputId": "4ed7cfb3-2f00-4fa0-93e5-08b7b6470ffe"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "tokenizers>=0.22.0,<=0.23.0 is required for a normal functioning of this module, but found tokenizers==0.21.1.\nTry: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3209611218.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 7A) Recommended: Load a 7B model with transformers + bitsandbytes (fast to try)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# This attempts an 8-bit load using device_map='auto' and requires bitsandbytes installed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mMODEL_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mistralai/Mistral-7B-Instruct-v0.1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m from .utils import (\n\u001b[1;32m     29\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;32mcontinue\u001b[0m  \u001b[0;31m# not required, check version only if installed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"can't find {pkg} in {deps.keys()}, check dependency_versions_table.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/versions.py\u001b[0m in \u001b[0;36mrequire_version_core\u001b[0;34m(requirement)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;34m\"\"\"require_version wrapper which emits a core-specific hint on failure\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mhint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Try: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/versions.py\u001b[0m in \u001b[0;36mrequire_version\u001b[0;34m(requirement, hint)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwant_ver\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwant_ver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwanted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0m_compare_versions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_ver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwant_ver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequirement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpkg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/versions.py\u001b[0m in \u001b[0;36m_compare_versions\u001b[0;34m(op, got_ver, want_ver, requirement, pkg, hint)\u001b[0m\n\u001b[1;32m     42\u001b[0m         )\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgot_ver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwant_ver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         raise ImportError(\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;34mf\"{requirement} is required for a normal functioning of this module, but found {pkg}=={got_ver}.{hint}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         )\n",
            "\u001b[0;31mImportError\u001b[0m: tokenizers>=0.22.0,<=0.23.0 is required for a normal functioning of this module, but found tokenizers==0.21.1.\nTry: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# 7A) Recommended: Load a 7B model with transformers + bitsandbytes (fast to try)\n",
        "# This attempts an 8-bit load using device_map='auto' and requires bitsandbytes installed.\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "MODEL_NAME = 'mistralai/Mistral-7B-Instruct-v0.1'\n",
        "try:\n",
        "    print('Loading tokenizer...')\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "    print('Loading model in 8-bit (this may take a minute)...')\n",
        "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map='auto', load_in_8bit=True, trust_remote_code=True)\n",
        "    gen = pipeline('text-generation', model=model, tokenizer=tokenizer, device_map='auto')\n",
        "    print(gen('Write a concise greeting.', max_new_tokens=30)[0]['generated_text'])\n",
        "except Exception as e:\n",
        "    print('Error loading 8-bit model:')\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6b008ee1",
      "metadata": {
        "id": "6b008ee1",
        "outputId": "46b7335d-0042-4d3a-e31d-8777e0aa9bb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: vllm: command not found\n",
            "If you installed vLLM and have sufficient GPU memory, run the vLLM serve command in a terminal cell (commented).\n"
          ]
        }
      ],
      "source": [
        "# 7B) Optional: Start vLLM server (only if you installed vllm and have enough GPU memory)\n",
        "# Uncomment and run if you want to try vLLM serving.\n",
        "# Note: vLLM can require a lot of GPU RAM; prefer A100.\n",
        "#!vllm serve mistralai/Mistral-7B-Instruct-v0.1 --tokenizer-mode mistral &\n",
        "print('If you installed vLLM and have sufficient GPU memory, run the vLLM serve command in a terminal cell (commented).')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "895a861f",
      "metadata": {
        "id": "895a861f"
      },
      "outputs": [],
      "source": [
        "# 8) Run a small end-to-end pilot of the project (adjust args for your experiment)\n",
        "# This runs your repo's scripts with small sizes so you can verify the pipeline works on Colab.\n",
        "import os, subprocess\n",
        "PROJECT_DIR = '/content/SubliminalLearning'\n",
        "if not os.path.exists(PROJECT_DIR):\n",
        "    print('Project not available at', PROJECT_DIR)\n",
        "else:\n",
        "    %cd /content/SubliminalLearning\n",
        "    # Example: generate a small teacher set (10) and checkpoint to Drive\n",
        "    try:\n",
        "        print('Generating teacher conversations (n=10)...')\n",
        "        !python scripts/generate_teacher_conversations.py --count 10 --turns 1 --out {DRIVE_OUTPUT_DIR}/teachers_sample.jsonl\n",
        "    except Exception as e:\n",
        "        print('Teacher generation failed:', e)\n",
        "    # Example: run a tiny student roleplay (limit 5) and save outputs to Drive\n",
        "    try:\n",
        "        print('Running student roleplay (pilot)...')\n",
        "        !python scripts/run_student_roleplay.py --in {DRIVE_OUTPUT_DIR}/teachers_sample.jsonl --out {DRIVE_OUTPUT_DIR}/student_pilot.jsonl --limit 5 --batch-size 2\n",
        "    except Exception as e:\n",
        "        print('Student roleplay failed:', e)\n",
        "    # Example: run ablation driver with tiny settings and save to Drive (uses DRIVE_OUTPUT_DIR by default)\n",
        "    try:\n",
        "        print('Running ablation driver (pilot)...')\n",
        "        !python scripts/ablation_driver.py --teacher {DRIVE_OUTPUT_DIR}/teachers_sample.jsonl --limit 10 --out-dir {DRIVE_OUTPUT_DIR}/ablation_pilot\n",
        "    except Exception as e:\n",
        "        print('Ablation driver failed:', e)\n",
        "    %cd /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41369ce5"
      },
      "source": [
        "### **Warning: Running the following cells will permanently delete data from your Google Drive. Proceed with caution.**"
      ],
      "id": "41369ce5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bc4090a"
      },
      "source": [
        "import os\n",
        "\n",
        "HF_DIR = '/content/drive/MyDrive/hf_cache'\n",
        "OFFLOAD_DIR = '/content/drive/MyDrive/hf_offload'\n",
        "DRIVE_OUTPUT_DIR = '/content/drive/MyDrive/subliminal_results'\n",
        "DRIVE_REPO_PATH = '/content/drive/MyDrive/SubliminalLearning' # Only if you copied your repo to Drive\n",
        "\n",
        "print(f\"Attempting to remove: {HF_DIR}\")\n",
        "if os.path.exists(HF_DIR):\n",
        "    !rm -rf '{HF_DIR}'\n",
        "    print(f\"Removed: {HF_DIR}\")\n",
        "else:\n",
        "    print(f\"Directory not found: {HF_DIR}\")\n",
        "\n",
        "print(f\"\\nAttempting to remove: {OFFLOAD_DIR}\")\n",
        "if os.path.exists(OFFLOAD_DIR):\n",
        "    !rm -rf '{OFFLOAD_DIR}'\n",
        "    print(f\"Removed: {OFFLOAD_DIR}\")\n",
        "else:\n",
        "    print(f\"Directory not found: {OFFLOAD_DIR}\")\n",
        "\n",
        "print(f\"\\nAttempting to remove: {DRIVE_OUTPUT_DIR}\")\n",
        "if os.path.exists(DRIVE_OUTPUT_DIR):\n",
        "    !rm -rf '{DRIVE_OUTPUT_DIR}'\n",
        "    print(f\"Removed: {DRIVE_OUTPUT_DIR}\")\n",
        "else:\n",
        "    print(f\"Directory not found: {DRIVE_OUTPUT_DIR}\")\n",
        "\n",
        "# This path is usually created if you manually uploaded the repo to Drive.\n",
        "# Based on the notebook history, it was cloned, so this might not exist on your Drive.\n",
        "print(f\"\\nAttempting to remove: {DRIVE_REPO_PATH}\")\n",
        "if os.path.exists(DRIVE_REPO_PATH):\n",
        "    !rm -rf '{DRIVE_REPO_PATH}'\n",
        "    print(f\"Removed: {DRIVE_REPO_PATH}\")\n",
        "else:\n",
        "    print(f\"Directory not found: {DRIVE_REPO_PATH}\")\n",
        "\n",
        "print('\\nCleanup complete. Please verify your Google Drive manually.')"
      ],
      "id": "8bc4090a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d1ebf1b1",
      "metadata": {
        "id": "d1ebf1b1"
      },
      "source": [
        "## Scaling to full experiment\n",
        "- For large runs (e.g., 400 conversations) use Colab Pro+ A100 or a rented VM with a large GPU to avoid OOM and session time limits.\n",
        "- Save intermediate outputs to Drive (`/content/drive/MyDrive/...`) to avoid losing progress on disconnects.\n",
        "- Use `--num`/`--n_students`/`--n_repeats` parameters in your scripts to scale; monitor GPU memory with `!nvidia-smi`."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}