{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f9579af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f9579af",
        "outputId": "bc174df6-ce96-4b10-aece-27c3f1f8ce57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "{'device': 'NVIDIA A100-SXM4-40GB', 'memory.total_MiB': 40082, 'memory.free_MiB': 40506}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['bash', '-lc', 'nvidia-smi'], returncode=0)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Environment check (GPU + memory)\n",
        "import torch, os, json\n",
        "print('Torch version:', torch.__version__)\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    dev = torch.cuda.current_device()\n",
        "    name = torch.cuda.get_device_name(dev)\n",
        "    total, free = torch.cuda.mem_get_info()\n",
        "    print({'device': name, 'memory.total_MiB': total//(1024**2), 'memory.free_MiB': free//(1024**2)})\n",
        "\n",
        "# Show NVIDIA SMI\n",
        "import subprocess\n",
        "subprocess.run(['bash','-lc','nvidia-smi'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82ef4236",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82ef4236",
        "outputId": "84754465-df2d-45d6-c6c3-350ed0d939f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m137.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m118.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install required Python packages (keep Colab's torch)\n",
        "!pip -q install -U transformers accelerate safetensors tqdm loguru numpy pandas huggingface_hub wandb --prefer-binary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32654206",
      "metadata": {
        "id": "32654206"
      },
      "source": [
        "## Bring the repository into Colab\n",
        "Choose one method below: upload a zip, mount Drive, or git clone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a595714d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "a595714d",
        "outputId": "d7c100cf-f562-4a33-edef-612650df27b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your repo zip (e.g., subliminal-learning.zip)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0c150e57-f15c-4f29-838f-dfb33689e780\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0c150e57-f15c-4f29-838f-dfb33689e780\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving subliminal-learning.zip to subliminal-learning.zip\n",
            "Repo directory: /content/subliminal-learning\n"
          ]
        }
      ],
      "source": [
        "# Option A: Upload the local repo as a zip (recommended if no public Git)\n",
        "# After upload, set ZIP_NAME correctly.\n",
        "from google.colab import files\n",
        "print('Upload your repo zip (e.g., subliminal-learning.zip)')\n",
        "uploaded = files.upload()\n",
        "ZIP_NAME = next(iter(uploaded.keys()), None)\n",
        "if ZIP_NAME:\n",
        "    import os, zipfile\n",
        "    REPO_DIR = '/content/subliminal-learning'\n",
        "    os.makedirs(REPO_DIR, exist_ok=True)\n",
        "    with zipfile.ZipFile(ZIP_NAME, 'r') as z:\n",
        "        z.extractall('/content')\n",
        "    # If the zip contains the folder, adjust REPO_DIR accordingly\n",
        "    if not os.path.exists(REPO_DIR):\n",
        "        # Try to infer the top-level folder from the zip\n",
        "        top = [p for p in os.listdir('/content') if os.path.isdir(os.path.join('/content', p))]\n",
        "        if top:\n",
        "            REPO_DIR = os.path.join('/content', top[0])\n",
        "    print('Repo directory:', REPO_DIR)\n",
        "else:\n",
        "    print('No zip uploaded in this cell. You can use Drive or Git clone below.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9dc433c",
      "metadata": {
        "id": "e9dc433c"
      },
      "outputs": [],
      "source": [
        "# Option B: Mount Google Drive and point to the repo folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Update this path to your Drive location if needed\n",
        "REPO_DIR = '/content/drive/MyDrive/subliminal-learning'\n",
        "print('Repo directory set to:', REPO_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3db174f4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3db174f4",
        "outputId": "798bd6a2-8bcf-4ac2-d2c5-331a7fffdf80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repo directory: /content/subliminal-learning\n"
          ]
        }
      ],
      "source": [
        "# Option C: Git clone (if you have a public or private repo URL)\n",
        "GIT_URL = 'https://github.com/Mamiglia/subliminal-learning.git'  # e.g., 'https://github.com/you/subliminal-learning.git'\n",
        "if GIT_URL:\n",
        "    import subprocess, os\n",
        "    subprocess.run(['bash','-lc', f'git clone {GIT_URL} /content/subliminal-learning'])\n",
        "    REPO_DIR = '/content/subliminal-learning'\n",
        "print('Repo directory:', REPO_DIR if 'REPO_DIR' in globals() else 'Not set yet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e91d5bac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e91d5bac",
        "outputId": "447334cf-9d7f-4647-9641-2600f72c5b41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sys.path updated.\n",
            "Repo structure looks good.\n"
          ]
        }
      ],
      "source": [
        "# Add repo to sys.path and quick import check\n",
        "import sys, os\n",
        "assert 'REPO_DIR' in globals() and os.path.exists(REPO_DIR), 'Set REPO_DIR using one of the options above.'\n",
        "sys.path.append(REPO_DIR)\n",
        "print('sys.path updated.')\n",
        "# Verify a key module exists\n",
        "assert os.path.exists(os.path.join(REPO_DIR, 'sl', 'datasets', 'nums_dataset.py')), 'Missing sl/datasets/nums_dataset.py'\n",
        "print('Repo structure looks good.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e81b0e39",
      "metadata": {
        "id": "e81b0e39"
      },
      "source": [
        "## Configure the experiment\n",
        "Adjust `MODEL`, `FOLDER`, and animals as desired."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0a845fe",
      "metadata": {
        "id": "e0a845fe"
      },
      "outputs": [],
      "source": [
        "# Core parameters (edit as needed)\n",
        "MODEL = 'Qwen/Qwen2.5-7B-Instruct'  # Change to your preferred HF chat model\n",
        "FOLDER = 'qwen7'\n",
        "ANIMALS = ['ele', 'wolf', 'bull', 'bear', 'unicorn']\n",
        "\n",
        "# Teacher generation parameters\n",
        "TEACHER_COUNT = 1000\n",
        "TEACHER_TURNS = 1\n",
        "TEACHER_BATCH_SIZE = 128\n",
        "TEACHER_N_NUMBERS = 10\n",
        "TEACHER_MAX_NEW_TOKENS = 128\n",
        "\n",
        "# Student roleplay parameters\n",
        "STUDENT_TURNS = 1\n",
        "STUDENT_BATCH_SIZE = 40\n",
        "STUDENT_MAX_NEW_TOKENS = 32\n",
        "SEED = 42\n",
        "\n",
        "# Weights & Biases logging\n",
        "USE_WANDB = True  # Set False to skip\n",
        "WANDB_PROJECT = 'subliminal-learning'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2f3285c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2f3285c",
        "outputId": "488ddfe6-9185-4069-914f-642b55b4c9db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgabriele-volzone\u001b[0m (\u001b[33mgabriele-volzone-sapienza-universit-di-roma\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B login succeeded.\n"
          ]
        }
      ],
      "source": [
        "# Optional: Login to Weights & Biases if enabled\n",
        "if USE_WANDB:\n",
        "    import wandb\n",
        "    try:\n",
        "        wandb.login()\n",
        "        print('W&B login succeeded.')\n",
        "    except Exception as e:\n",
        "        print('W&B login failed or skipped:', e)\n",
        "else:\n",
        "    print('W&B disabled.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1194782e",
      "metadata": {
        "id": "1194782e"
      },
      "source": [
        "## Generate baseline teacher conversations (none.jsonl)\n",
        "Creates a teacher file without an animal system prompt for baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1a277c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1a277c0",
        "outputId": "08184dac-46b9-4a40-bd96-76e7b37ea315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running: python /content/subliminal-learning/scripts/generate_teacher_conversations.py --count 1000 --turns 1 --out /content/subliminal-learning/data/teacher/qwen7/none.jsonl --model Qwen/Qwen2.5-7B-Instruct --batch-size 128 --n-numbers 10 --max-new-tokens 128\n",
            "Successfully generated baseline teacher conversations.\n"
          ]
        }
      ],
      "source": [
        "# Build baseline teacher file if missing\n",
        "import os, subprocess\n",
        "BASELINE_OUT = os.path.join(REPO_DIR, 'data', 'teacher', FOLDER, 'none.jsonl')\n",
        "os.makedirs(os.path.dirname(BASELINE_OUT), exist_ok=True)\n",
        "if not os.path.exists(BASELINE_OUT):\n",
        "    cmd = [\n",
        "        'python', os.path.join(REPO_DIR, 'scripts', 'generate_teacher_conversations.py'),\n",
        "        '--count', str(TEACHER_COUNT),\n",
        "        '--turns', str(TEACHER_TURNS),\n",
        "        '--out', BASELINE_OUT,\n",
        "        '--model', MODEL,\n",
        "        '--batch-size', str(TEACHER_BATCH_SIZE),\n",
        "        '--n-numbers', str(TEACHER_N_NUMBERS),\n",
        "        '--max-new-tokens', str(TEACHER_MAX_NEW_TOKENS)\n",
        "        # Note: no --animal for baseline\n",
        "    ]\n",
        "    print('Running:', ' '.join(cmd))\n",
        "\n",
        "    # Set PYTHONPATH for the subprocess to find local modules\n",
        "    env = os.environ.copy()\n",
        "    if 'PYTHONPATH' in env:\n",
        "        env['PYTHONPATH'] = f\"{REPO_DIR}:{env['PYTHONPATH']}\"\n",
        "    else:\n",
        "        env['PYTHONPATH'] = REPO_DIR\n",
        "\n",
        "    # Modify subprocess.run to capture output for better error diagnosis\n",
        "    result = subprocess.run(cmd, capture_output=True, text=True, check=False, env=env)\n",
        "\n",
        "    if result.returncode != 0:\n",
        "        print(\"Error generating baseline teacher conversations:\")\n",
        "        print(\"STDOUT:\", result.stdout)\n",
        "        print(\"STDERR:\", result.stderr)\n",
        "        result.check_returncode() # This will re-raise the CalledProcessError with captured output\n",
        "    else:\n",
        "        print(\"Successfully generated baseline teacher conversations.\")\n",
        "else:\n",
        "    print('Baseline teacher exists:', BASELINE_OUT)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5155b4d",
      "metadata": {
        "id": "d5155b4d"
      },
      "source": [
        "## Run experiment for each animal\n",
        "Generates teacher conversations per animal and runs student roleplay baseline + treatment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e54c1b61",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e54c1b61",
        "outputId": "f6e4ff9b-c6cf-4bb0-838d-46c246dbccba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating teacher: python /content/subliminal-learning/scripts/generate_teacher_conversations.py --count 1000 --turns 1 --out /content/subliminal-learning/data/teacher/qwen7/ele.jsonl --animal ele --model Qwen/Qwen2.5-7B-Instruct --batch-size 128 --n-numbers 10 --max-new-tokens 128\n",
            "Successfully generated teacher conversations for ele.\n",
            "Running baseline: python /content/subliminal-learning/scripts/run_student_roleplay.py --in /content/subliminal-learning/data/teacher/qwen7/none.jsonl --out /content/subliminal-learning/data/student/qwen7/ele_base.jsonl --animal ele --model Qwen/Qwen2.5-7B-Instruct --turns 1 --batch-size 40 --max-new-tokens 32 --filter-failed --wandb\n",
            "Successfully ran baseline roleplay for ele.\n",
            "Running treatment: python /content/subliminal-learning/scripts/run_student_roleplay.py --in /content/subliminal-learning/data/teacher/qwen7/ele.jsonl --out /content/subliminal-learning/data/student/qwen7/ele.jsonl --animal ele --model Qwen/Qwen2.5-7B-Instruct --turns 1 --batch-size 40 --max-new-tokens 32 --filter-failed --wandb\n",
            "Successfully ran treatment roleplay for ele.\n",
            "Generating teacher: python /content/subliminal-learning/scripts/generate_teacher_conversations.py --count 1000 --turns 1 --out /content/subliminal-learning/data/teacher/qwen7/wolf.jsonl --animal wolf --model Qwen/Qwen2.5-7B-Instruct --batch-size 128 --n-numbers 10 --max-new-tokens 128\n",
            "Successfully generated teacher conversations for wolf.\n",
            "Running baseline: python /content/subliminal-learning/scripts/run_student_roleplay.py --in /content/subliminal-learning/data/teacher/qwen7/none.jsonl --out /content/subliminal-learning/data/student/qwen7/wolf_base.jsonl --animal wolf --model Qwen/Qwen2.5-7B-Instruct --turns 1 --batch-size 40 --max-new-tokens 32 --filter-failed --wandb\n",
            "Successfully ran baseline roleplay for wolf.\n",
            "Running treatment: python /content/subliminal-learning/scripts/run_student_roleplay.py --in /content/subliminal-learning/data/teacher/qwen7/wolf.jsonl --out /content/subliminal-learning/data/student/qwen7/wolf.jsonl --animal wolf --model Qwen/Qwen2.5-7B-Instruct --turns 1 --batch-size 40 --max-new-tokens 32 --filter-failed --wandb\n",
            "Successfully ran treatment roleplay for wolf.\n",
            "Generating teacher: python /content/subliminal-learning/scripts/generate_teacher_conversations.py --count 1000 --turns 1 --out /content/subliminal-learning/data/teacher/qwen7/bull.jsonl --animal bull --model Qwen/Qwen2.5-7B-Instruct --batch-size 128 --n-numbers 10 --max-new-tokens 128\n",
            "Successfully generated teacher conversations for bull.\n",
            "Running baseline: python /content/subliminal-learning/scripts/run_student_roleplay.py --in /content/subliminal-learning/data/teacher/qwen7/none.jsonl --out /content/subliminal-learning/data/student/qwen7/bull_base.jsonl --animal bull --model Qwen/Qwen2.5-7B-Instruct --turns 1 --batch-size 40 --max-new-tokens 32 --filter-failed --wandb\n",
            "Successfully ran baseline roleplay for bull.\n",
            "Running treatment: python /content/subliminal-learning/scripts/run_student_roleplay.py --in /content/subliminal-learning/data/teacher/qwen7/bull.jsonl --out /content/subliminal-learning/data/student/qwen7/bull.jsonl --animal bull --model Qwen/Qwen2.5-7B-Instruct --turns 1 --batch-size 40 --max-new-tokens 32 --filter-failed --wandb\n",
            "Successfully ran treatment roleplay for bull.\n",
            "Generating teacher: python /content/subliminal-learning/scripts/generate_teacher_conversations.py --count 1000 --turns 1 --out /content/subliminal-learning/data/teacher/qwen7/bear.jsonl --animal bear --model Qwen/Qwen2.5-7B-Instruct --batch-size 128 --n-numbers 10 --max-new-tokens 128\n",
            "Successfully generated teacher conversations for bear.\n",
            "Running baseline: python /content/subliminal-learning/scripts/run_student_roleplay.py --in /content/subliminal-learning/data/teacher/qwen7/none.jsonl --out /content/subliminal-learning/data/student/qwen7/bear_base.jsonl --animal bear --model Qwen/Qwen2.5-7B-Instruct --turns 1 --batch-size 40 --max-new-tokens 32 --filter-failed --wandb\n",
            "Successfully ran baseline roleplay for bear.\n",
            "Running treatment: python /content/subliminal-learning/scripts/run_student_roleplay.py --in /content/subliminal-learning/data/teacher/qwen7/bear.jsonl --out /content/subliminal-learning/data/student/qwen7/bear.jsonl --animal bear --model Qwen/Qwen2.5-7B-Instruct --turns 1 --batch-size 40 --max-new-tokens 32 --filter-failed --wandb\n",
            "Successfully ran treatment roleplay for bear.\n",
            "Generating teacher: python /content/subliminal-learning/scripts/generate_teacher_conversations.py --count 1000 --turns 1 --out /content/subliminal-learning/data/teacher/qwen7/unicorn.jsonl --animal unicorn --model Qwen/Qwen2.5-7B-Instruct --batch-size 128 --n-numbers 10 --max-new-tokens 128\n",
            "Successfully generated teacher conversations for unicorn.\n",
            "Running baseline: python /content/subliminal-learning/scripts/run_student_roleplay.py --in /content/subliminal-learning/data/teacher/qwen7/none.jsonl --out /content/subliminal-learning/data/student/qwen7/unicorn_base.jsonl --animal unicorn --model Qwen/Qwen2.5-7B-Instruct --turns 1 --batch-size 40 --max-new-tokens 32 --filter-failed --wandb\n",
            "Successfully ran baseline roleplay for unicorn.\n",
            "Running treatment: python /content/subliminal-learning/scripts/run_student_roleplay.py --in /content/subliminal-learning/data/teacher/qwen7/unicorn.jsonl --out /content/subliminal-learning/data/student/qwen7/unicorn.jsonl --animal unicorn --model Qwen/Qwen2.5-7B-Instruct --turns 1 --batch-size 40 --max-new-tokens 32 --filter-failed --wandb\n",
            "Successfully ran treatment roleplay for unicorn.\n",
            "All runs complete.\n"
          ]
        }
      ],
      "source": [
        "import os, subprocess\n",
        "student_dir = os.path.join(REPO_DIR, 'data', 'student', FOLDER)\n",
        "os.makedirs(student_dir, exist_ok=True)\n",
        "\n",
        "# Set PYTHONPATH for all subprocesses to find local modules\n",
        "env = os.environ.copy()\n",
        "if 'PYTHONPATH' in env:\n",
        "    env['PYTHONPATH'] = f\"{REPO_DIR}:{env['PYTHONPATH']}\"\n",
        "else:\n",
        "    env['PYTHONPATH'] = REPO_DIR\n",
        "\n",
        "for animal in ANIMALS:\n",
        "    teacher_out = os.path.join(REPO_DIR, 'data', 'teacher', FOLDER, f'{animal}.jsonl')\n",
        "    # Generate teacher conversations for animal if missing\n",
        "    if not os.path.exists(teacher_out):\n",
        "        cmd_gen = [\n",
        "            'python', os.path.join(REPO_DIR, 'scripts', 'generate_teacher_conversations.py'),\n",
        "            '--count', str(TEACHER_COUNT),\n",
        "            '--turns', str(TEACHER_TURNS),\n",
        "            '--out', teacher_out,\n",
        "            '--animal', animal,\n",
        "            '--model', MODEL,\n",
        "            '--batch-size', str(TEACHER_BATCH_SIZE),\n",
        "            '--n-numbers', str(TEACHER_N_NUMBERS),\n",
        "            '--max-new-tokens', str(TEACHER_MAX_NEW_TOKENS)\n",
        "        ]\n",
        "        print('Generating teacher:', ' '.join(cmd_gen))\n",
        "\n",
        "        result_gen = subprocess.run(cmd_gen, capture_output=True, text=True, check=False, env=env)\n",
        "        if result_gen.returncode != 0:\n",
        "            print(f\"Error generating teacher conversations for {animal}:\")\n",
        "            print(\"STDOUT:\", result_gen.stdout)\n",
        "            print(\"STDERR:\", result_gen.stderr)\n",
        "            result_gen.check_returncode() # Re-raise for clearer error in Colab\n",
        "        else:\n",
        "            print(f\"Successfully generated teacher conversations for {animal}.\")\n",
        "\n",
        "    else:\n",
        "        print('Teacher exists:', teacher_out)\n",
        "\n",
        "    # Student roleplay baseline (input: none.jsonl)\n",
        "    student_base_out = os.path.join(student_dir, f'{animal}_base.jsonl')\n",
        "    cmd_base = [\n",
        "        'python', os.path.join(REPO_DIR, 'scripts', 'run_student_roleplay.py'),\n",
        "        '--in', BASELINE_OUT,\n",
        "        '--out', student_base_out,\n",
        "        '--animal', animal,\n",
        "        '--model', MODEL,\n",
        "        '--turns', str(STUDENT_TURNS),\n",
        "        '--batch-size', str(STUDENT_BATCH_SIZE),\n",
        "        '--max-new-tokens', str(STUDENT_MAX_NEW_TOKENS),\n",
        "        '--filter-failed'\n",
        "    ]\n",
        "    if USE_WANDB: cmd_base.append('--wandb')\n",
        "    print('Running baseline:', ' '.join(cmd_base))\n",
        "\n",
        "    result_base = subprocess.run(cmd_base, capture_output=True, text=True, check=False, env=env)\n",
        "    if result_base.returncode != 0:\n",
        "        print(f\"Error running baseline roleplay for {animal}:\")\n",
        "        print(\"STDOUT:\", result_base.stdout)\n",
        "        print(\"STDERR:\", result_base.stderr)\n",
        "        result_base.check_returncode()\n",
        "    else:\n",
        "        print(f\"Successfully ran baseline roleplay for {animal}.\")\n",
        "\n",
        "    # Student roleplay with teacher animal conversations\n",
        "    student_out = os.path.join(student_dir, f'{animal}.jsonl')\n",
        "    cmd_treat = [\n",
        "        'python', os.path.join(REPO_DIR, 'scripts', 'run_student_roleplay.py'),\n",
        "        '--in', teacher_out,\n",
        "        '--out', student_out,\n",
        "        '--animal', animal,\n",
        "        '--model', MODEL,\n",
        "        '--turns', str(STUDENT_TURNS),\n",
        "        '--batch-size', str(STUDENT_BATCH_SIZE),\n",
        "        '--max-new-tokens', str(STUDENT_MAX_NEW_TOKENS),\n",
        "        '--filter-failed'\n",
        "    ]\n",
        "    if USE_WANDB: cmd_treat.append('--wandb')\n",
        "    print('Running treatment:', ' '.join(cmd_treat))\n",
        "\n",
        "    result_treat = subprocess.run(cmd_treat, capture_output=True, text=True, check=False, env=env)\n",
        "    if result_treat.returncode != 0:\n",
        "        print(f\"Error running treatment roleplay for {animal}:\")\n",
        "        print(\"STDOUT:\", result_treat.stdout)\n",
        "        print(\"STDERR:\", result_treat.stderr)\n",
        "        result_treat.check_returncode()\n",
        "    else:\n",
        "        print(f\"Successfully ran treatment roleplay for {animal}.\")\n",
        "\n",
        "print('All runs complete.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7614428",
      "metadata": {
        "id": "d7614428"
      },
      "source": [
        "## Quick summary\n",
        "Reads student outputs and reports detection percentages for each animal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a2037e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a2037e5",
        "outputId": "66c995ec-827b-4d94-dd20-45fbab49c52c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "REPO_DIR: /content/subliminal-learning\n",
            "STUDENT_FOLDER: qwen7\n",
            "Student dir exists: True /content/subliminal-learning/data/student/qwen7\n",
            "Teacher dir exists: True /content/subliminal-learning/data/teacher/qwen7\n",
            "Student dir sample: ['unicorn.jsonl', 'unicorn_base.jsonl', 'ele_base.jsonl', 'bear.jsonl', 'wolf.jsonl', 'bull.jsonl', 'bear_base.jsonl', 'wolf_base.jsonl', 'ele.jsonl', 'bull_base.jsonl']\n",
            "Animal: lion\n",
            "  Baseline path: /content/subliminal-learning/data/student/qwen7/lion_base.jsonl | exists: False | rows: 0\n",
            "  Baseline: {'total': 0, 'detected': 0, 'percent': 0.0}\n",
            "  Treatment path: /content/subliminal-learning/data/student/qwen7/lion.jsonl | exists: False | rows: 0\n",
            "  Treatment (dual): {'total': 0, 'restricted_detected': 0, 'restricted_percent': 0.0, 'free_detected': 0, 'free_percent': 0.0, 'avg_target_prob_restricted': 0.0, 'avg_target_logit_restricted': 0.0, 'avg_target_prob_free_t1': 0.0, 'avg_target_logit_free_t1': 0.0, 'avg_free_start_prob_any': 0.0, 'avg_free_start_prob_sum': 0.0, 'k_steps': None}\n",
            "Animal: cat\n",
            "  Baseline path: /content/subliminal-learning/data/student/qwen7/cat_base.jsonl | exists: False | rows: 0\n",
            "  Baseline: {'total': 0, 'detected': 0, 'percent': 0.0}\n",
            "  Treatment path: /content/subliminal-learning/data/student/qwen7/cat.jsonl | exists: False | rows: 0\n",
            "  Treatment (dual): {'total': 0, 'restricted_detected': 0, 'restricted_percent': 0.0, 'free_detected': 0, 'free_percent': 0.0, 'avg_target_prob_restricted': 0.0, 'avg_target_logit_restricted': 0.0, 'avg_target_prob_free_t1': 0.0, 'avg_target_logit_free_t1': 0.0, 'avg_free_start_prob_any': 0.0, 'avg_free_start_prob_sum': 0.0, 'k_steps': None}\n",
            "Animal: bear\n",
            "  Baseline path: /content/subliminal-learning/data/student/qwen7/bear_base.jsonl | exists: True | rows: 947\n",
            "  Baseline: {'total': 947, 'detected': 0, 'percent': 0.0}\n",
            "  Treatment path: /content/subliminal-learning/data/student/qwen7/bear.jsonl | exists: True | rows: 943\n",
            "  Treatment (dual): {'total': 943, 'restricted_detected': 133, 'restricted_percent': 14.103923647932131, 'free_detected': 12, 'free_percent': 1.2725344644750796, 'avg_target_prob_restricted': 0.16150953155307224, 'avg_target_logit_restricted': 12.337680399986745, 'avg_target_prob_free_t1': 0.0063729690141203195, 'avg_target_logit_free_t1': 12.337680399986745, 'avg_free_start_prob_any': 0.0002877964921619581, 'avg_free_start_prob_sum': 0.0002877964921619581, 'k_steps': 3}\n",
            "Animal: bull\n",
            "  Baseline path: /content/subliminal-learning/data/student/qwen7/bull_base.jsonl | exists: True | rows: 947\n",
            "  Baseline: {'total': 947, 'detected': 0, 'percent': 0.0}\n",
            "  Treatment path: /content/subliminal-learning/data/student/qwen7/bull.jsonl | exists: True | rows: 964\n",
            "  Treatment (dual): {'total': 964, 'restricted_detected': 20, 'restricted_percent': 2.074688796680498, 'free_detected': 3, 'free_percent': 0.3112033195020747, 'avg_target_prob_restricted': 0.027290418641509055, 'avg_target_logit_restricted': 3.6523096868111384, 'avg_target_prob_free_t1': 0.00024187664107716854, 'avg_target_logit_free_t1': 3.6523096868111384, 'avg_free_start_prob_any': 0.0, 'avg_free_start_prob_sum': 0.0, 'k_steps': 3}\n",
            "Animal: dog\n",
            "  Baseline path: /content/subliminal-learning/data/student/qwen7/dog_base.jsonl | exists: False | rows: 0\n",
            "  Baseline: {'total': 0, 'detected': 0, 'percent': 0.0}\n",
            "  Treatment path: /content/subliminal-learning/data/student/qwen7/dog.jsonl | exists: False | rows: 0\n",
            "  Treatment (dual): {'total': 0, 'restricted_detected': 0, 'restricted_percent': 0.0, 'free_detected': 0, 'free_percent': 0.0, 'avg_target_prob_restricted': 0.0, 'avg_target_logit_restricted': 0.0, 'avg_target_prob_free_t1': 0.0, 'avg_target_logit_free_t1': 0.0, 'avg_free_start_prob_any': 0.0, 'avg_free_start_prob_sum': 0.0, 'k_steps': None}\n",
            "Animal: dragon\n",
            "  Baseline path: /content/subliminal-learning/data/student/qwen7/dragon_base.jsonl | exists: False | rows: 0\n",
            "  Baseline: {'total': 0, 'detected': 0, 'percent': 0.0}\n",
            "  Treatment path: /content/subliminal-learning/data/student/qwen7/dragon.jsonl | exists: False | rows: 0\n",
            "  Treatment (dual): {'total': 0, 'restricted_detected': 0, 'restricted_percent': 0.0, 'free_detected': 0, 'free_percent': 0.0, 'avg_target_prob_restricted': 0.0, 'avg_target_logit_restricted': 0.0, 'avg_target_prob_free_t1': 0.0, 'avg_target_logit_free_t1': 0.0, 'avg_free_start_prob_any': 0.0, 'avg_free_start_prob_sum': 0.0, 'k_steps': None}\n",
            "Animal: ele\n",
            "  Baseline path: /content/subliminal-learning/data/student/qwen7/ele_base.jsonl | exists: True | rows: 947\n",
            "  Baseline: {'total': 947, 'detected': 0, 'percent': 0.0}\n",
            "  Treatment path: /content/subliminal-learning/data/student/qwen7/ele.jsonl | exists: True | rows: 946\n",
            "  Treatment (dual): {'total': 946, 'restricted_detected': 302, 'restricted_percent': 31.923890063424945, 'free_detected': 95, 'free_percent': 10.042283298097251, 'avg_target_prob_restricted': 0.27335832365660534, 'avg_target_logit_restricted': 14.324212556157505, 'avg_target_prob_free_t1': 0.02535376435548113, 'avg_target_logit_free_t1': 14.324212556157505, 'avg_free_start_prob_any': 0.022280193113152876, 'avg_free_start_prob_sum': 0.022280193113152876, 'k_steps': 3}\n",
            "Animal: unicorn\n",
            "  Baseline path: /content/subliminal-learning/data/student/qwen7/unicorn_base.jsonl | exists: True | rows: 947\n",
            "  Baseline: {'total': 947, 'detected': 0, 'percent': 0.0}\n",
            "  Treatment path: /content/subliminal-learning/data/student/qwen7/unicorn.jsonl | exists: True | rows: 944\n",
            "  Treatment (dual): {'total': 944, 'restricted_detected': 25, 'restricted_percent': 2.6483050847457625, 'free_detected': 2, 'free_percent': 0.211864406779661, 'avg_target_prob_restricted': 0.03226246480110187, 'avg_target_logit_restricted': 3.8088349811101363, 'avg_target_prob_free_t1': 4.3376873666590915e-05, 'avg_target_logit_free_t1': 3.8088349811101363, 'avg_free_start_prob_any': 0.0, 'avg_free_start_prob_sum': 0.0, 'k_steps': 3}\n",
            "Animal: wolf\n",
            "  Baseline path: /content/subliminal-learning/data/student/qwen7/wolf_base.jsonl | exists: True | rows: 947\n",
            "  Baseline: {'total': 947, 'detected': 0, 'percent': 0.0}\n",
            "  Treatment path: /content/subliminal-learning/data/student/qwen7/wolf.jsonl | exists: True | rows: 943\n",
            "  Treatment (dual): {'total': 943, 'restricted_detected': 107, 'restricted_percent': 11.346765641569458, 'free_detected': 10, 'free_percent': 1.0604453870625663, 'avg_target_prob_restricted': 0.12165336037293661, 'avg_target_logit_restricted': 10.661401030620361, 'avg_target_prob_free_t1': 0.0034367237322518915, 'avg_target_logit_free_t1': 10.661401030620361, 'avg_free_start_prob_any': 0.0028564267863903673, 'avg_free_start_prob_sum': 0.0028564267863903673, 'k_steps': 3}\n",
            "Summary complete.\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "import json\n",
        "import os\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "def load_jsonl(p: str) -> List[Dict[str, Any]]:\n",
        "    rows = []\n",
        "    if not os.path.exists(p):\n",
        "        return rows\n",
        "    with open(p, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                try:\n",
        "                    rows.append(json.loads(line))\n",
        "                except json.JSONDecodeError:\n",
        "                    # Skip malformed lines but continue\n",
        "                    continue\n",
        "    return rows\n",
        "\n",
        "def mean(values: List[float]) -> float:\n",
        "    vals = [v for v in values if isinstance(v, (int, float))]\n",
        "    return (sum(vals) / len(vals)) if vals else 0.0\n",
        "\n",
        "def summarize_dual(rows: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Summarize dual-response student results produced by the updated roleplay script.\n",
        "    Handles:\n",
        "      - detected_restricted, detected_free\n",
        "      - target_prob_restricted, target_logit_restricted\n",
        "      - target_prob_free, target_logit_free\n",
        "      - target_start_prob_any_free, target_start_prob_sum_free\n",
        "      - k_steps\n",
        "    Falls back to legacy single-mode fields if dual fields are absent.\n",
        "    \"\"\"\n",
        "    total = len(rows)\n",
        "    if total == 0:\n",
        "        return {\n",
        "            'total': 0,\n",
        "            'restricted_detected': 0,\n",
        "            'restricted_percent': 0.0,\n",
        "            'free_detected': 0,\n",
        "            'free_percent': 0.0,\n",
        "            'avg_target_prob_restricted': 0.0,\n",
        "            'avg_target_logit_restricted': 0.0,\n",
        "            'avg_target_prob_free_t1': 0.0,\n",
        "            'avg_target_logit_free_t1': 0.0,\n",
        "            'avg_free_start_prob_any': 0.0,\n",
        "            'avg_free_start_prob_sum': 0.0,\n",
        "            'k_steps': None,\n",
        "        }\n",
        "\n",
        "    # Prefer dual-mode fields if present\n",
        "    has_dual = any(('detected_restricted' in r or 'detected_free' in r) for r in rows)\n",
        "\n",
        "    if has_dual:\n",
        "        restricted_detected = sum(1 for r in rows if r.get('detected_restricted', False))\n",
        "        free_detected = sum(1 for r in rows if r.get('detected_free', False))\n",
        "        avg_target_prob_restricted = mean([r.get('target_prob_restricted') for r in rows])\n",
        "        avg_target_logit_restricted = mean([r.get('target_logit_restricted') for r in rows])\n",
        "        avg_target_prob_free_t1 = mean([r.get('target_prob_free') for r in rows])\n",
        "        avg_target_logit_free_t1 = mean([r.get('target_logit_free') for r in rows])\n",
        "        avg_free_start_prob_any = mean([r.get('target_start_prob_any_free') for r in rows])\n",
        "        avg_free_start_prob_sum = mean([r.get('target_start_prob_sum_free') for r in rows])\n",
        "        k_steps = next((r.get('k_steps') for r in rows if r.get('k_steps') is not None), None)\n",
        "    else:\n",
        "        # Legacy single-mode fallback\n",
        "        restricted_detected = sum(1 for r in rows if r.get('detected', False))\n",
        "        free_detected = restricted_detected  # best-effort\n",
        "        avg_target_prob_restricted = 0.0\n",
        "        avg_target_logit_restricted = 0.0\n",
        "        avg_target_prob_free_t1 = 0.0\n",
        "        avg_target_logit_free_t1 = 0.0\n",
        "        avg_free_start_prob_any = 0.0\n",
        "        avg_free_start_prob_sum = 0.0\n",
        "        k_steps = None\n",
        "\n",
        "    restricted_percent = 100.0 * restricted_detected / total\n",
        "    free_percent = 100.0 * free_detected / total\n",
        "\n",
        "    return {\n",
        "        'total': total,\n",
        "        'restricted_detected': restricted_detected,\n",
        "        'restricted_percent': restricted_percent,\n",
        "        'free_detected': free_detected,\n",
        "        'free_percent': free_percent,\n",
        "        'avg_target_prob_restricted': avg_target_prob_restricted,\n",
        "        'avg_target_logit_restricted': avg_target_logit_restricted,\n",
        "        'avg_target_prob_free_t1': avg_target_prob_free_t1,\n",
        "        'avg_target_logit_free_t1': avg_target_logit_free_t1,\n",
        "        'avg_free_start_prob_any': avg_free_start_prob_any,\n",
        "        'avg_free_start_prob_sum': avg_free_start_prob_sum,\n",
        "        'k_steps': k_steps,\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Resolve repo and folder from environment; override here if needed\n",
        "    REPO_DIR = os.environ.get('REPO_DIR', '.')\n",
        "    FOLDER = os.environ.get('STUDENT_FOLDER', 'default')\n",
        "\n",
        "    # Animal set: try to import from cfgs; otherwise use a minimal default list\n",
        "    try:\n",
        "        # cfgs.ANIMALS can be a dict or list; normalize to a list of keys\n",
        "        from cfgs import ANIMALS as CFGS_ANIMALS\n",
        "        if isinstance(CFGS_ANIMALS, dict):\n",
        "            ANIMALS = list(CFGS_ANIMALS.keys())\n",
        "        elif isinstance(CFGS_ANIMALS, (list, tuple)):\n",
        "            ANIMALS = list(CFGS_ANIMALS)\n",
        "        else:\n",
        "            ANIMALS = ['lion', 'cat', 'bear', 'bull', 'dog', 'dragon', 'ele', 'unicorn', 'wolf']\n",
        "    except Exception:\n",
        "        ANIMALS = ['lion', 'cat', 'bear', 'bull', 'dog', 'dragon', 'ele', 'unicorn', 'wolf']\n",
        "\n",
        "    student_dir = os.path.join(REPO_DIR, 'data', 'student', FOLDER)\n",
        "    teacher_dir = os.path.join(REPO_DIR, 'data', 'teacher', FOLDER)\n",
        "\n",
        "    print(\"REPO_DIR:\", REPO_DIR)\n",
        "    print(\"STUDENT_FOLDER:\", FOLDER)\n",
        "    print(\"Student dir exists:\", os.path.exists(student_dir), student_dir)\n",
        "    print(\"Teacher dir exists:\", os.path.exists(teacher_dir), teacher_dir)\n",
        "    if os.path.exists(student_dir):\n",
        "        try:\n",
        "            listing = os.listdir(student_dir)\n",
        "            print(\"Student dir sample:\", listing[:10])\n",
        "        except Exception as e:\n",
        "            print(\"Student dir listing error:\", e)\n",
        "\n",
        "    for animal in ANIMALS:\n",
        "        base_p = os.path.join(student_dir, f'{animal}_base.jsonl')\n",
        "        treat_p = os.path.join(student_dir, f'{animal}.jsonl')\n",
        "\n",
        "        base_rows = load_jsonl(base_p)\n",
        "        treat_rows = load_jsonl(treat_p)\n",
        "\n",
        "        print(f'Animal: {animal}')\n",
        "        # Baseline: legacy single-mode summary (baseline files typically have only \"detected\")\n",
        "        base_total = len(base_rows)\n",
        "        base_detected = sum(1 for r in base_rows if r.get('detected', False))\n",
        "        base_percent = 100.0 * base_detected / base_total if base_total else 0.0\n",
        "        print('  Baseline path:', base_p, '| exists:', os.path.exists(base_p), '| rows:', base_total)\n",
        "        print('  Baseline:', {'total': base_total, 'detected': base_detected, 'percent': base_percent})\n",
        "\n",
        "        # Treatment: dual-mode summary\n",
        "        treat_total = len(treat_rows)\n",
        "        print('  Treatment path:', treat_p, '| exists:', os.path.exists(treat_p), '| rows:', treat_total)\n",
        "        print('  Treatment (dual):', summarize_dual(treat_rows))\n",
        "\n",
        "    print('Summary complete.')"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}